{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ecb9d23",
   "metadata": {},
   "source": [
    "# Heart Pre_Processing\n",
    "\n",
    "In this notebook we pre-process the heart dataset we are using as well as generating synthetic datasets.\n",
    "\n",
    "SynthVAE is suitable for example datasets in which there are no time series variables. The heart dataset we use in heart.csv is given through a kaggle challenge found here <https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction> and is a combination of multiple open source datasets.\n",
    "\n",
    "In order to run this notebook we need access to:\n",
    "\n",
    "- training SynthVAE\n",
    "- original heart.csv\n",
    "- adapted RDT module for reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126d0410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import torch\n",
    "\n",
    "# For VAE dataset formatting\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Opacus support for differential privacy\n",
    "from opacus.utils.uniform_sampler import UniformWithReplacementSampler\n",
    "\n",
    "from VAE import VAE, Encoder, Decoder\n",
    "\n",
    "from utils import general_pre_proc, reverse_transformers, set_seed\n",
    "\n",
    "#import warnings\n",
    "\n",
    "#warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7476fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9026fddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the heart csv and perform pre-processing\n",
    "\n",
    "heart_data = pd.read_csv(\"Heart_Data/Original_Data/heart.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae27a6e",
   "metadata": {},
   "source": [
    "## Pre-Processing Steps\n",
    "\n",
    "We need to transform the continuous & categorical columns accordingly for synthetic data creation - this is different to the pre-processing required for the actual predictive modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eec942f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify column configs\n",
    "\n",
    "categorical_columns = [\n",
    "    \"Sex\",\n",
    "    \"ChestPainType\",\n",
    "    \"FastingBS\",\n",
    "    \"RestingECG\",\n",
    "    \"ExerciseAngina\",\n",
    "    \"ST_Slope\",\n",
    "    \"HeartDisease\",\n",
    "]\n",
    "continuous_columns = [\"Age\", \"RestingBP\", \"Cholesterol\", \"MaxHR\", \"Oldpeak\"]\n",
    "pre_proc_method = \"GMM\"\n",
    "\n",
    "(\n",
    "    x_train,\n",
    "    original_metric_set,\n",
    "    reordered_dataframe_columns,\n",
    "    continuous_transformers,\n",
    "    categorical_transformers,\n",
    "    datetime_transformers,\n",
    "    num_categories,\n",
    "    num_continuous,\n",
    ") = general_pre_proc(\n",
    "    data_supp=heart_data,\n",
    "    user_categorical_columns=categorical_columns,\n",
    "    user_continuous_columns=continuous_columns,\n",
    "    user_datetime_columns=[],\n",
    "    pre_proc_method=pre_proc_method,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3895a6e7",
   "metadata": {},
   "source": [
    "# Synthetic Data Creation\n",
    "\n",
    "Here we train and generate synthetic data using SynthVAE - we set a number of seeds and create multiple versions which are then saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dfaa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_seeds = 5  # Number of synthetic sets we want to create & test\n",
    "\n",
    "for repeat_number in range(n_seeds):\n",
    "\n",
    "    #%% -------- Create & Train VAE -------- #\n",
    "\n",
    "    # User defined hyperparams\n",
    "    # General training\n",
    "    batch_size = 32\n",
    "    latent_dim = 256\n",
    "    hidden_dim = 256\n",
    "    n_epochs = 100\n",
    "    logging_freq = 1  # Number of epochs we should log the results to the user\n",
    "    patience = 5  # How many epochs should we allow the model train to see if\n",
    "    # improvement is made\n",
    "    delta = 10  # The difference between elbo values that registers an improvement\n",
    "    filepath = None  # Where to save the best model\n",
    "\n",
    "    # Privacy params\n",
    "    differential_privacy = False  # Do we want to implement differential privacy\n",
    "    sample_rate = 0.1  # Sampling rate\n",
    "    C = 1e16  # Clipping threshold - any gradients above this are clipped\n",
    "    noise_scale = None  # Noise multiplier - influences how much noise to add\n",
    "    target_eps = 10  # Target epsilon for privacy accountant\n",
    "    target_delta = 1e-3  # Target delta for privacy accountant\n",
    "\n",
    "    # Prepare data for interaction with torch VAE\n",
    "    Y = torch.Tensor(x_train)\n",
    "    dataset = TensorDataset(Y)\n",
    "\n",
    "    generator = None\n",
    "    sample_rate = batch_size / len(dataset)\n",
    "    data_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_sampler=UniformWithReplacementSampler(\n",
    "            num_samples=len(dataset), sample_rate=sample_rate, generator=generator\n",
    "        ),\n",
    "        pin_memory=True,\n",
    "        generator=generator,\n",
    "    )\n",
    "\n",
    "    # Create VAE\n",
    "\n",
    "    encoder = Encoder(x_train.shape[1], latent_dim, hidden_dim=hidden_dim)\n",
    "    decoder = Decoder(latent_dim, num_continuous, num_categories=num_categories)\n",
    "\n",
    "    vae = VAE(encoder, decoder)\n",
    "\n",
    "    if differential_privacy == False:\n",
    "        (\n",
    "            training_epochs,\n",
    "            log_elbo,\n",
    "            log_reconstruction,\n",
    "            log_divergence,\n",
    "            log_categorical,\n",
    "            log_numerical,\n",
    "        ) = vae.train(data_loader, n_epochs=n_epochs)\n",
    "\n",
    "    elif differential_privacy == True:\n",
    "        (\n",
    "            training_epochs,\n",
    "            log_elbo,\n",
    "            log_reconstruction,\n",
    "            log_divergence,\n",
    "            log_categorical,\n",
    "            log_numerical,\n",
    "        ) = vae.diff_priv_train(\n",
    "            data_loader,\n",
    "            n_epochs=n_epochs,\n",
    "            C=C,\n",
    "            target_eps=target_eps,\n",
    "            target_delta=target_delta,\n",
    "            sample_rate=sample_rate,\n",
    "            noise_scale=noise_scale,\n",
    "        )\n",
    "        print(f\"(epsilon, delta): {vae.get_privacy_spent(target_delta)}\")\n",
    "\n",
    "    synthetic_sample = vae.generate(heart_data.shape[0])\n",
    "\n",
    "    # Reverse the transformations\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        synthetic_sample = pd.DataFrame(\n",
    "            synthetic_sample.cpu().detach().numpy(), columns=reordered_dataframe_columns\n",
    "        )\n",
    "    else:\n",
    "        synthetic_sample = pd.DataFrame(\n",
    "            synthetic_sample.detach().numpy(), columns=reordered_dataframe_columns\n",
    "        )\n",
    "\n",
    "    synthetic_supp = reverse_transformers(\n",
    "        synthetic_set=synthetic_sample,\n",
    "        data_supp_columns=heart_data.columns,\n",
    "        cont_transformers=continuous_transformers,\n",
    "        cat_transformers=categorical_transformers,\n",
    "        date_transformers=None,\n",
    "        pre_proc_method=pre_proc_method,\n",
    "    )\n",
    "\n",
    "    if(differential_privacy==False):\n",
    "        synthetic_supp.to_csv(\"Heart_Data/Synthetic_Data/No_DP/synthetic_heart_run_{}.csv\".format(repeat_number), index=False)\n",
    "    else:\n",
    "        synthetic_supp.to_csv(\"Heart_Data/Synthetic_Data/DP/synthetic_heart_run_{}.csv\".format(repeat_number), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

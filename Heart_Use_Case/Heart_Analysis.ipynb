{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Analysis\n",
    "\n",
    "In this notebook we are showing an exemplar of SynthVAE in good practise.\n",
    "\n",
    "SynthVAE is suitable for example datasets in which there are no time series variables. The heart dataset we use in heart.csv is given through a kaggle challenge found here <https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction> and is a combination of multiple open source datasets.\n",
    "\n",
    "In order to run this notebook we need access to:\n",
    "\n",
    "- training SynthVAE\n",
    "- heart data folder (with synthetic data ran using the Heart_Pre_Processing.ipynb)\n",
    "- adapted RDT module for reproducible results\n",
    "\n",
    "We create a 10-fold split validation method. We then compare results between synthetic data created by SynthVAE & the original dataset. We will be comparing distributional metrics as well as downstream ML performance.\n",
    "\n",
    "We opt to use decision trees as they are white box and openly interpretable - this will allow us to fully observe if trends captured in the original dataset are also captured within the synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "import sklearn.preprocessing\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "import sys\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import torch\n",
    "\n",
    "# For VAE dataset formatting\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Opacus support for differential privacy\n",
    "from opacus.utils.uniform_sampler import UniformWithReplacementSampler\n",
    "\n",
    "from VAE import VAE, Encoder, Decoder\n",
    "\n",
    "from utils import set_seed, plot_variable_distributions\n",
    "\n",
    "from metrics import distribution_metrics, privacy_metrics\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify column configs\n",
    "\n",
    "categorical_columns = [\n",
    "    \"Sex\",\n",
    "    \"ChestPainType\",\n",
    "    \"FastingBS\",\n",
    "    \"RestingECG\",\n",
    "    \"ExerciseAngina\",\n",
    "    \"ST_Slope\",\n",
    "    \"HeartDisease\",\n",
    "]\n",
    "\n",
    "continuous_columns = [\n",
    "    \"Age\", \n",
    "    \"RestingBP\", \n",
    "    \"Cholesterol\", \n",
    "    \"MaxHR\", \n",
    "    \"Oldpeak\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_file = \"Heart_Data/Original_Data/heart.csv\"\n",
    "\n",
    "heart_dataset = pd.read_csv(heart_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop over datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_no_dp = \"Heart_Data/Synthetic_Data/No_DP/\" # directory for all your synthetic data - without differential privacy\n",
    "data_dir_dp = \"Heart_Data/Synthetic_Data/DP/\" # Directory for all your synthetic data - with differential privacy\n",
    "\n",
    "file_paths_dp = [os.path.join(data_dir_dp, file) for file in listdir(data_dir_dp)]\n",
    "file_paths_no_dp = [os.path.join(data_dir_no_dp, file) for file in listdir(data_dir_no_dp)]\n",
    "\n",
    "files = [heart_file] + file_paths_no_dp # + file_paths_dp\n",
    "\n",
    "os.mkdir(\"Results\")\n",
    "\n",
    "for i, file in enumerate(files):\n",
    "    \n",
    "    save_file_name = file[file.rindex('/')+1:]\n",
    "    \n",
    "    # Create folder/directory for results of this dataset\n",
    "    \n",
    "    os.mkdir(\"Results/{}\".format(save_file_name[:-4]))\n",
    "    \n",
    "    run_folder = \"Results/{}\".format(save_file_name[:-4])\n",
    "    \n",
    "    dataset = pd.read_csv(file) # Read in dataset as pandas dataframe\n",
    "    \n",
    "    X,y = dataset.loc[:, [\"Age\", \"Sex\", \"ChestPainType\", \"RestingBP\", \"Cholesterol\", \"FastingBS\", \"RestingECG\", \"MaxHR\", \"ExerciseAngina\", \"Oldpeak\", \"ST_Slope\"]], dataset.loc[:, \"HeartDisease\"]\n",
    "    \n",
    "    # define the model & pipeline\n",
    "    numeric_transformer = StandardScaler()\n",
    "\n",
    "    categorical_transformer = OneHotEncoder()\n",
    "\n",
    "    preprocessor_variables = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, continuous_columns),\n",
    "            (\"cat\", categorical_transformer, categorical_columns[:-1]),\n",
    "        ], sparse_threshold=0,\n",
    "    )\n",
    "\n",
    "    # configure the cross-validation procedure\n",
    "    cv_outer = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=1)\n",
    "\n",
    "    outer_test_results = []\n",
    "    outer_train_results = []\n",
    "    outer_configs = []\n",
    "\n",
    "    for train_ix, test_ix in cv_outer.split(X, y):\n",
    "    \n",
    "        # split data\n",
    "        X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n",
    "        y_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
    "    \n",
    "        # configure the cross-validation procedure\n",
    "        cv_inner = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
    "    \n",
    "        # define the model\n",
    "        pipe = Pipeline([('scaler', preprocessor_variables), ('model', DecisionTreeClassifier(random_state=0))])\n",
    "    \n",
    "        # define search space\n",
    "        param_grid = dict(model__max_depth=[1,2,3], model__criterion=['gini', 'entropy'], model__splitter=['best', 'random'])\n",
    "    \n",
    "        # define search\n",
    "        search = GridSearchCV(pipe, param_grid, scoring='accuracy', cv=cv_inner, refit=True, verbose=0)\n",
    "    \n",
    "        # execute search\n",
    "        result = search.fit(X_train, y_train)\n",
    "    \n",
    "        # get the best performing model fit on the whole training set\n",
    "        best_model = result.best_estimator_\n",
    "    \n",
    "        # evaluate model on the hold out dataset\n",
    "        yhat = best_model.predict(X_test)\n",
    "    \n",
    "        # evaluate the model\n",
    "        acc = accuracy_score(y_test, yhat)\n",
    "    \n",
    "        # store the all relevant results\n",
    "        outer_test_results.append(acc)\n",
    "        outer_train_results.append(result.best_score_)\n",
    "        outer_configs.append(result.best_params_)\n",
    "    \n",
    "        # report progress\n",
    "        print('>acc=%.3f, est=%.3f, cfg=%s' % (acc, result.best_score_, result.best_params_))\n",
    "    # summarize the estimated performance of the model\n",
    "    print('Accuracy: %.3f (%.3f)' % (np.mean(outer_test_results), np.std(outer_test_results)))\n",
    "    \n",
    "    # Save important results\n",
    "    \n",
    "    with open( (run_folder + \"/test_score_results.pkl\"), \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(outer_test_results, fp)\n",
    "        \n",
    "    with open( (run_folder + \"/train_score_results.pkl\"), \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(outer_train_results, fp)\n",
    "    \n",
    "    with open( (run_folder + \"/hyperparameter_configs.pkl\"), \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(outer_configs, fp)\n",
    "        \n",
    "    # Now we need to save tree diagrams for each model as well as variable importances\n",
    "    # define the model using the best hyperparameters\n",
    "    clf = Pipeline([('scaler', preprocessor_variables), ('model', DecisionTreeClassifier(random_state=0, max_depth=result.best_params_['model__max_depth'], splitter=result.best_params_['model__splitter'], criterion=result.best_params_['model__criterion']))])\n",
    "    clf = clf.fit(X, y)\n",
    "    \n",
    "    plt.figure(figsize=(40,20))\n",
    "    tree.plot_tree(clf['model'])\n",
    "    plt.savefig(run_folder + \"/tree_diagram.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    feature_importances = clf['model'].feature_importances_\n",
    "    result = permutation_importance(\n",
    "        clf, X, y, n_repeats=10, random_state=42\n",
    "    )\n",
    "    permutation_importances = result.importances_mean\n",
    "    \n",
    "    np.save( (run_folder + \"/feature_importances.npy\"), feature_importances )\n",
    "    np.save( (run_folder + \"/permutation_importances.npy\"), permutation_importances)\n",
    "    \n",
    "    \n",
    "    if(i!=0): # If we are using the synthetic sets - lets calculate distributional & privacy metrics\n",
    "        \n",
    "        user_metrics = [\n",
    "            \"SVCDetection\",\n",
    "            \"GMLogLikelihood\",\n",
    "            \"CSTest\",\n",
    "            \"KSTest\",\n",
    "            \"KSTestExtended\",\n",
    "            \"ContinuousKLDivergence\",\n",
    "            \"DiscreteKLDivergence\",\n",
    "        ]\n",
    "        \n",
    "        distributional_metrics = distribution_metrics(\n",
    "            gower_bool=False,\n",
    "            distributional_metrics=user_metrics,\n",
    "            data_supp=heart_dataset,\n",
    "            synthetic_supp=dataset,\n",
    "            categorical_columns=categorical_columns,\n",
    "            continuous_columns=continuous_columns,\n",
    "            saving_filepath=\"\",\n",
    "            pre_proc_method=\"GMM\",\n",
    "        )\n",
    "        \n",
    "        plot_variable_distributions(\n",
    "            categorical_columns=categorical_columns,\n",
    "            continuous_columns=continuous_columns,\n",
    "            data_supp=heart_dataset,\n",
    "            synthetic_supp=dataset,\n",
    "            saving_filepath=\"\",\n",
    "            pre_proc_method=\"GMM\",\n",
    "        )\n",
    "        \n",
    "        # Save the distributional metrics as csv\n",
    "        distributional_metrics.to_csv(run_folder + \"/distributional_metrics.csv\")\n",
    "        \n",
    "        privacy_metric = privacy_metrics(\n",
    "            private_variable=\"Sex\",\n",
    "            data_supp=heart_dataset,\n",
    "            synthetic_supp=dataset,\n",
    "            categorical_columns=categorical_columns,\n",
    "            continuous_columns=continuous_columns,\n",
    "            saving_filepath=None,\n",
    "            pre_proc_method=\"GMM\",\n",
    "        )\n",
    "        \n",
    "        # Save the privacy results\n",
    "        with open( (run_folder + \"/privacy_metric.pkl\"), \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(privacy_metric, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering Original Fidelity Results\n",
    "\n",
    "Now we run our detection tree model on the original heart dataset to get an initial breakdown of results:\n",
    "\n",
    "- We are running a 10 fold stratified validation method that is repeated three times using shuffling for each fold\n",
    "- We are using sklearn decision trees\n",
    "- We pre-process after the splitting has been done\n",
    "- We set the <b>random_state</b> for reproducible data splits each time & we are using stratified to ensure roughly equivalent distributions of heart event to non-heart event in each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = heart_data.loc[:, [\"Age\", \"Sex\", \"ChestPainType\", \"RestingBP\", \"Cholesterol\", \"FastingBS\", \"RestingECG\", \"MaxHR\", \"ExerciseAngina\", \"Oldpeak\", \"ST_Slope\"]], heart_data.loc[:, \"HeartDisease\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model & pipeline\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "categorical_transformer = OrdinalEncoder()\n",
    "\n",
    "preprocessor_variables = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, continuous_columns),\n",
    "        (\"cat\", categorical_transformer, categorical_columns[:-1]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# configure the cross-validation procedure\n",
    "cv_outer = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=1)\n",
    "\n",
    "outer_test_results = []\n",
    "outer_train_results = []\n",
    "outer_configs = []\n",
    "\n",
    "for train_ix, test_ix in cv_outer.split(X, y):\n",
    "    \n",
    "    # split data\n",
    "    X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n",
    "    y_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
    "    \n",
    "    # configure the cross-validation procedure\n",
    "    cv_inner = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
    "    \n",
    "    # define the model\n",
    "    pipe = Pipeline([('scaler', preprocessor_variables), ('model', DecisionTreeClassifier(random_state=0))])\n",
    "    \n",
    "    # define search space\n",
    "    param_grid = dict(model__max_depth=[1,2,3], model__criterion=['gini', 'entropy'], model__splitter=['best', 'random'])\n",
    "    \n",
    "    # define search\n",
    "    search = GridSearchCV(pipe, param_grid, scoring='accuracy', cv=cv_inner, refit=True, verbose=0)\n",
    "    \n",
    "    # execute search\n",
    "    result = search.fit(X_train, y_train)\n",
    "    \n",
    "    # get the best performing model fit on the whole training set\n",
    "    best_model = result.best_estimator_\n",
    "    \n",
    "    # evaluate model on the hold out dataset\n",
    "    yhat = best_model.predict(X_test)\n",
    "    \n",
    "    # evaluate the model\n",
    "    acc = accuracy_score(y_test, yhat)\n",
    "    \n",
    "    # store the all relevant results\n",
    "    outer_test_results.append(acc)\n",
    "    outer_train_results.append(result.best_score_)\n",
    "    outer_configs.append(result.best_params_)\n",
    "    \n",
    "    # report progress\n",
    "    print('>acc=%.3f, est=%.3f, cfg=%s' % (acc, result.best_score_, result.best_params_))\n",
    "# summarize the estimated performance of the model\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(outer_results), np.std(outer_results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "\n",
    "Here we start creation of our scikit tree model. We decide to use tree based models as they offer greatly improved interpretability. This will allow us to look at feature importance breakdowns at classification stage and make comparisons between synthetic & original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model using the best hyperparameters\n",
    "clf = Pipeline([('scaler', preprocessor_variables), ('model', DecisionTreeClassifier(random_state=0, max_depth=3, splitter='best'))])\n",
    "clf = clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Feature Importance\n",
    "\n",
    "Now lets look at the decision graphs to identify interpretable cutoff points in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40,20))\n",
    "tree.plot_tree(clf['model'])\n",
    "plt.savefig(\"tree_diagram_{}.png\".format(dataset))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = clf['model'].feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = permutation_importance(\n",
    "    clf, X, y, n_repeats=10, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_importances = result.importances_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributional Metrics\n",
    "\n",
    "For synthetic sets we want to also save any distributional/statistical metrics of interest between the synthetic/real datasers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Privacy Metrics\n",
    "\n",
    "For synthetic sets we also want to observe how privacy differences influence the workflow & data fidelity"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a3faeeb7a141a1b6863ef3d83f2d4891432bfa1117b17a94d8e78eaa2bfb2ea7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
